{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22c9732e-c501-43b6-8b31-b8ce85916426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nextstate (generic function with 1 method)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 6.6 in Sutton Barto\n",
    "\n",
    "# number the state space starting from the upper left hand corner.\n",
    "# there are 48 states (possible positions). state 37 is the start\n",
    "# and state 48 is the goal. rewards are -1 except states 38 thru\n",
    "# 47 give a reward of -100 and send the agent back to state 37.\n",
    "\n",
    "# actions are numbered 1,2,3,4 = up,down,right,left\n",
    "\n",
    "function reward(s, a)  # return reward for taking action a from state s\n",
    "    if s == goal\n",
    "        return 0\n",
    "    elseif (s == start && a == 3) || (in(s, 26:35) && a == 2)\n",
    "        return -100   # stepped into the cliff\n",
    "    else\n",
    "        return -1\n",
    "    end\n",
    "end\n",
    "\n",
    "function nextstate(s, a) # return next state when taking action a from state s\n",
    "    if (s == goal) || (s == 36 && a == 2)\n",
    "        return goal\n",
    "    elseif (s == start && a == 3) || (in(s, 26:35) && a == 2)\n",
    "        return start   # stepped into the cliff so back to start\n",
    "    elseif in(s, [ 1 13 25 37 ]) && a == 4  # step off grid to left\n",
    "        return s\n",
    "    elseif in(s, 1:12) && a == 1    # step off grid above\n",
    "        return s\n",
    "    elseif in(s, [ 12 24 36 ]) && a == 3   # step off grid to right\n",
    "        return s\n",
    "    elseif in(s, 1:11) && a == 3 # step to right\n",
    "        return s + 1\n",
    "    elseif in(s, 2:12) && a == 4\n",
    "        return s - 1\n",
    "    elseif in(s, 13:23) && a == 3 # step to right\n",
    "        return s + 1\n",
    "    elseif in(s, 14:24) && a == 4\n",
    "        return s - 1\n",
    "    elseif in(s, 25:35) && a == 3\n",
    "        return s + 1\n",
    "    elseif in(s, 26:36) && a == 4\n",
    "        return s - 1\n",
    "    elseif in(s, 1:24) && a == 2\n",
    "        return s + 12\n",
    "    elseif in(s, 13:36) && a == 1\n",
    "        return s - 12\n",
    "    elseif s == start && a == 2\n",
    "        return s\n",
    "    elseif s == start && a == 1\n",
    "        return 25\n",
    "    elseif s == 25 && a == 2\n",
    "        return start\n",
    "    else\n",
    "        println(\"!!! unexpected state $(s) and action $(a) !!!\")\n",
    "        @assert false\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3628ba1f-24f7-4a76-8b06-5ee655620f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cliff Directions!!:\n",
      "[3, 2, 2, 2, 2, 3, 1, 3, 2, 1, 3, 2]\n",
      "[4, 3, 1, 3, 3, 3, 3, 3, 3, 3, 2, 2]\n",
      "[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "states = 48\n",
    "actions = 4\n",
    "alpha = 0.1\n",
    "gamma = 1.0\n",
    "epsilon = 0.1\n",
    "start = 37\n",
    "goal = 48\n",
    "episodes = 500\n",
    "\n",
    "\n",
    "function q_learning()\n",
    "    states = 48  \n",
    "    actions = 4  \n",
    "    \n",
    "    Q = zeros((states, actions))\n",
    "\n",
    "    for episode in 1:episodes\n",
    "        state = start\n",
    "        while state != goal\n",
    "            if rand() < epsilon\n",
    "                action = rand(1:actions)\n",
    "            else\n",
    "                action = argmax(Q[state, :])\n",
    "            end\n",
    "            \n",
    "            next_state = nextstate(state, action)  \n",
    "            r = reward(state, action)  \n",
    "            \n",
    "            Q[state, action] += alpha * (r + gamma * maximum(Q[next_state, :]) - Q[state, action])\n",
    "            state = next_state\n",
    "        end\n",
    "    end\n",
    "\n",
    "    policy = zeros(Int, (4, 12))\n",
    "    for s in 1:states\n",
    "        policy[(s - 1) รท 12 + 1, (s - 1) % 12 + 1] = argmax(Q[s, :])\n",
    "    end\n",
    "    \n",
    "    println(\"Cliff Directions!!:\")\n",
    "    for row in 1:4\n",
    "        println(policy[row, :])\n",
    "    end\n",
    "end\n",
    "\n",
    "q_learning()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8db28f8-c1a7-4eaa-8bd5-11971ce75edb",
   "metadata": {},
   "source": [
    "Reward and nextstates are provided by Prof. Darin England. I implement the qlearning function. This follows the pseudocode provied on page 131 of sutton-barto, and outputs a \"cliff walking\" list where each number corresponds to a direction - 1=up, 2=down, 3=right, 4=left. I chose to use the same number of epsiodes that was used in the sutton-barto example, 500, and start and goal were defined in the question. The function loops through each episode, and while the state doesnt eqaul the goal value, it chooses a epsilon greedy policy to chose its policy. It then uses the provided functions to take the action. After that, Q[state, action]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
